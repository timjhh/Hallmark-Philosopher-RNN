{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import requests as rq\n",
    "import sys\n",
    "import io\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import keras\n",
    "import keras.callbacks\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"nietzsche.txt\", encoding=\"utf-8\")\n",
    "text_niet = file.read()\n",
    "# 732020:732176\n",
    "\n",
    "file_date = open(\"baddate.txt\", encoding=\"utf-8\")\n",
    "text_date = file_date.read()\n",
    "\n",
    "EPOCHS = 50\n",
    "diversity = 0.5\n",
    "\n",
    "# Do we not\n",
    "# hear the noise of the grave-diggers who are burying God? Do we not smell\n",
    "# the divine putrefaction?â€”for even Gods putrefy! God is dead! God remains\n",
    "# dead! And we have killed him!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    \"\"\"Helper function to sample an index from a probability array.\"\"\"\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.exp(np.log(preds) / temperature)\n",
    "    preds = preds / np.sum(preds)               \n",
    "    probas = np.random.multinomial(1, preds, 1) \n",
    "    return np.argmax(probas)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timjh\\AppData\\Local\\Temp\\ipykernel_8520\\1356494578.py:11: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  x = np.zeros((len(sentences), seqlen, len(chars)), dtype=np.bool)\n",
      "C:\\Users\\timjh\\AppData\\Local\\Temp\\ipykernel_8520\\1356494578.py:12: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y = np.zeros((len(sentences), seqlen, len(chars)), dtype=np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "666/667 [============================>.] - ETA: 0s - loss: 2.0099 - categorical_crossentropy: 2.0099 - accuracy: 0.4286\n",
      "----- Generating text after Epoch: 0\n",
      "667/667 [==============================] - 93s 136ms/step - loss: 2.0097 - categorical_crossentropy: 2.0097 - accuracy: 0.4286\n",
      "Epoch 2/50\n",
      "666/667 [============================>.] - ETA: 0s - loss: 1.5755 - categorical_crossentropy: 1.5755 - accuracy: 0.5378\n",
      "----- Generating text after Epoch: 1\n",
      "667/667 [==============================] - 88s 132ms/step - loss: 1.5755 - categorical_crossentropy: 1.5755 - accuracy: 0.5378\n",
      "Epoch 3/50\n",
      "666/667 [============================>.] - ETA: 0s - loss: 1.5127 - categorical_crossentropy: 1.5127 - accuracy: 0.5534\n",
      "----- Generating text after Epoch: 2\n",
      "667/667 [==============================] - 93s 139ms/step - loss: 1.5126 - categorical_crossentropy: 1.5126 - accuracy: 0.5534\n",
      "Epoch 4/50\n",
      "666/667 [============================>.] - ETA: 0s - loss: 1.4845 - categorical_crossentropy: 1.4845 - accuracy: 0.5597\n",
      "----- Generating text after Epoch: 3\n",
      "667/667 [==============================] - 86s 128ms/step - loss: 1.4845 - categorical_crossentropy: 1.4845 - accuracy: 0.5597\n",
      "Epoch 5/50\n",
      "666/667 [============================>.] - ETA: 0s - loss: 1.4656 - categorical_crossentropy: 1.4656 - accuracy: 0.5639\n",
      "----- Generating text after Epoch: 4\n",
      "667/667 [==============================] - 91s 136ms/step - loss: 1.4656 - categorical_crossentropy: 1.4656 - accuracy: 0.5639\n",
      "Epoch 6/50\n",
      "666/667 [============================>.] - ETA: 0s - loss: 1.4527 - categorical_crossentropy: 1.4527 - accuracy: 0.5671\n",
      "----- Generating text after Epoch: 5\n",
      "667/667 [==============================] - 99s 148ms/step - loss: 1.4528 - categorical_crossentropy: 1.4528 - accuracy: 0.5671\n",
      "Epoch 7/50\n",
      "666/667 [============================>.] - ETA: 0s - loss: 1.4442 - categorical_crossentropy: 1.4442 - accuracy: 0.5691\n",
      "----- Generating text after Epoch: 6\n",
      "667/667 [==============================] - 118s 178ms/step - loss: 1.4442 - categorical_crossentropy: 1.4442 - accuracy: 0.5691\n",
      "Epoch 8/50\n",
      "666/667 [============================>.] - ETA: 0s - loss: 1.4371 - categorical_crossentropy: 1.4371 - accuracy: 0.5711\n",
      "----- Generating text after Epoch: 7\n",
      "667/667 [==============================] - 132s 197ms/step - loss: 1.4371 - categorical_crossentropy: 1.4371 - accuracy: 0.5711\n",
      "Epoch 9/50\n",
      "667/667 [==============================] - ETA: 0s - loss: 1.4319 - categorical_crossentropy: 1.4319 - accuracy: 0.5721\n",
      "----- Generating text after Epoch: 8\n",
      "667/667 [==============================] - 215s 322ms/step - loss: 1.4319 - categorical_crossentropy: 1.4319 - accuracy: 0.5721\n",
      "Epoch 10/50\n",
      "667/667 [==============================] - ETA: 0s - loss: 1.4272 - categorical_crossentropy: 1.4272 - accuracy: 0.5734\n",
      "----- Generating text after Epoch: 9\n",
      "667/667 [==============================] - 229s 344ms/step - loss: 1.4272 - categorical_crossentropy: 1.4272 - accuracy: 0.5734\n",
      "Epoch 11/50\n",
      "667/667 [==============================] - ETA: 0s - loss: 1.4236 - categorical_crossentropy: 1.4236 - accuracy: 0.5742\n",
      "----- Generating text after Epoch: 10\n",
      "667/667 [==============================] - 241s 362ms/step - loss: 1.4236 - categorical_crossentropy: 1.4236 - accuracy: 0.5742\n",
      "Epoch 12/50\n",
      "666/667 [============================>.] - ETA: 0s - loss: 1.4205 - categorical_crossentropy: 1.4205 - accuracy: 0.5750\n",
      "----- Generating text after Epoch: 11\n",
      "667/667 [==============================] - 230s 345ms/step - loss: 1.4205 - categorical_crossentropy: 1.4205 - accuracy: 0.5750\n",
      "Epoch 13/50\n",
      "666/667 [============================>.] - ETA: 0s - loss: 1.4174 - categorical_crossentropy: 1.4174 - accuracy: 0.5757\n",
      "----- Generating text after Epoch: 12\n",
      "667/667 [==============================] - 246s 369ms/step - loss: 1.4174 - categorical_crossentropy: 1.4174 - accuracy: 0.5757\n",
      "Epoch 14/50\n",
      "666/667 [============================>.] - ETA: 0s - loss: 1.4150 - categorical_crossentropy: 1.4150 - accuracy: 0.5761\n",
      "----- Generating text after Epoch: 13\n",
      "667/667 [==============================] - 262s 392ms/step - loss: 1.4150 - categorical_crossentropy: 1.4150 - accuracy: 0.5761\n",
      "Epoch 15/50\n",
      "667/667 [==============================] - ETA: 0s - loss: 1.4129 - categorical_crossentropy: 1.4129 - accuracy: 0.5766\n",
      "----- Generating text after Epoch: 14\n",
      "667/667 [==============================] - 270s 405ms/step - loss: 1.4129 - categorical_crossentropy: 1.4129 - accuracy: 0.5766\n",
      "Epoch 16/50\n",
      "667/667 [==============================] - ETA: 0s - loss: 1.4109 - categorical_crossentropy: 1.4109 - accuracy: 0.5773\n",
      "----- Generating text after Epoch: 15\n",
      "667/667 [==============================] - 272s 408ms/step - loss: 1.4109 - categorical_crossentropy: 1.4109 - accuracy: 0.5773\n",
      "Epoch 17/50\n",
      "667/667 [==============================] - ETA: 0s - loss: 1.4090 - categorical_crossentropy: 1.4090 - accuracy: 0.5777\n",
      "----- Generating text after Epoch: 16\n",
      "667/667 [==============================] - 292s 438ms/step - loss: 1.4090 - categorical_crossentropy: 1.4090 - accuracy: 0.5777\n",
      "Epoch 18/50\n",
      "667/667 [==============================] - ETA: 0s - loss: 1.4075 - categorical_crossentropy: 1.4075 - accuracy: 0.5781\n",
      "----- Generating text after Epoch: 17\n",
      "667/667 [==============================] - 284s 426ms/step - loss: 1.4075 - categorical_crossentropy: 1.4075 - accuracy: 0.5781\n",
      "Epoch 19/50\n",
      "667/667 [==============================] - ETA: 0s - loss: 1.4057 - categorical_crossentropy: 1.4057 - accuracy: 0.5785\n",
      "----- Generating text after Epoch: 18\n",
      "667/667 [==============================] - 285s 428ms/step - loss: 1.4057 - categorical_crossentropy: 1.4057 - accuracy: 0.5785\n",
      "Epoch 20/50\n",
      "667/667 [==============================] - ETA: 0s - loss: 1.4042 - categorical_crossentropy: 1.4042 - accuracy: 0.5790\n",
      "----- Generating text after Epoch: 19\n",
      "667/667 [==============================] - 295s 442ms/step - loss: 1.4042 - categorical_crossentropy: 1.4042 - accuracy: 0.5790\n",
      "Epoch 21/50\n",
      "667/667 [==============================] - ETA: 0s - loss: 1.4031 - categorical_crossentropy: 1.4031 - accuracy: 0.5792\n",
      "----- Generating text after Epoch: 20\n",
      "667/667 [==============================] - 295s 442ms/step - loss: 1.4031 - categorical_crossentropy: 1.4031 - accuracy: 0.5792\n",
      "Epoch 22/50\n",
      "667/667 [==============================] - ETA: 0s - loss: 1.4020 - categorical_crossentropy: 1.4020 - accuracy: 0.5795\n",
      "----- Generating text after Epoch: 21\n",
      "667/667 [==============================] - 305s 458ms/step - loss: 1.4020 - categorical_crossentropy: 1.4020 - accuracy: 0.5795\n",
      "Epoch 23/50\n",
      "667/667 [==============================] - ETA: 0s - loss: 1.4007 - categorical_crossentropy: 1.4007 - accuracy: 0.5798\n",
      "----- Generating text after Epoch: 22\n",
      "667/667 [==============================] - 303s 455ms/step - loss: 1.4007 - categorical_crossentropy: 1.4007 - accuracy: 0.5798\n",
      "Epoch 24/50\n",
      "667/667 [==============================] - ETA: 0s - loss: 1.3997 - categorical_crossentropy: 1.3997 - accuracy: 0.5801\n",
      "----- Generating text after Epoch: 23\n",
      "667/667 [==============================] - 311s 466ms/step - loss: 1.3997 - categorical_crossentropy: 1.3997 - accuracy: 0.5801\n",
      "Epoch 25/50\n",
      "667/667 [==============================] - ETA: 0s - loss: 1.3988 - categorical_crossentropy: 1.3988 - accuracy: 0.5800\n",
      "----- Generating text after Epoch: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timjh\\AppData\\Local\\Temp\\ipykernel_8520\\748897970.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  preds = np.exp(np.log(preds) / temperature)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "667/667 [==============================] - 315s 472ms/step - loss: 1.3988 - categorical_crossentropy: 1.3988 - accuracy: 0.5800\n",
      "Epoch 26/50\n",
      "667/667 [==============================] - ETA: 0s - loss: 1.3978 - categorical_crossentropy: 1.3978 - accuracy: 0.5805\n",
      "----- Generating text after Epoch: 25\n",
      "667/667 [==============================] - 320s 479ms/step - loss: 1.3978 - categorical_crossentropy: 1.3978 - accuracy: 0.5805\n",
      "Epoch 27/50\n",
      "667/667 [==============================] - ETA: 0s - loss: 1.3971 - categorical_crossentropy: 1.3971 - accuracy: 0.5807\n",
      "----- Generating text after Epoch: 26\n",
      "667/667 [==============================] - 319s 478ms/step - loss: 1.3971 - categorical_crossentropy: 1.3971 - accuracy: 0.5807\n",
      "Epoch 28/50\n",
      "667/667 [==============================] - ETA: 0s - loss: 1.3962 - categorical_crossentropy: 1.3962 - accuracy: 0.5810\n",
      "----- Generating text after Epoch: 27\n",
      "667/667 [==============================] - 329s 494ms/step - loss: 1.3962 - categorical_crossentropy: 1.3962 - accuracy: 0.5810\n",
      "Epoch 29/50\n",
      "667/667 [==============================] - ETA: 0s - loss: 1.3956 - categorical_crossentropy: 1.3956 - accuracy: 0.5809\n",
      "----- Generating text after Epoch: 28\n",
      "667/667 [==============================] - 327s 491ms/step - loss: 1.3956 - categorical_crossentropy: 1.3956 - accuracy: 0.5809\n",
      "Epoch 30/50\n",
      "667/667 [==============================] - ETA: 0s - loss: 1.3949 - categorical_crossentropy: 1.3949 - accuracy: 0.5812\n",
      "----- Generating text after Epoch: 29\n",
      "667/667 [==============================] - 332s 498ms/step - loss: 1.3949 - categorical_crossentropy: 1.3949 - accuracy: 0.5812\n",
      "Epoch 31/50\n",
      "667/667 [==============================] - ETA: 0s - loss: 1.3943 - categorical_crossentropy: 1.3943 - accuracy: 0.5814\n",
      "----- Generating text after Epoch: 30\n",
      "667/667 [==============================] - 339s 508ms/step - loss: 1.3943 - categorical_crossentropy: 1.3943 - accuracy: 0.5814\n",
      "Epoch 32/50\n",
      "667/667 [==============================] - ETA: 0s - loss: 1.3935 - categorical_crossentropy: 1.3935 - accuracy: 0.5816\n",
      "----- Generating text after Epoch: 31\n",
      "667/667 [==============================] - 340s 510ms/step - loss: 1.3935 - categorical_crossentropy: 1.3935 - accuracy: 0.5816\n",
      "Epoch 33/50\n",
      "667/667 [==============================] - ETA: 0s - loss: 1.3930 - categorical_crossentropy: 1.3930 - accuracy: 0.5818\n",
      "----- Generating text after Epoch: 32\n",
      "667/667 [==============================] - 349s 524ms/step - loss: 1.3930 - categorical_crossentropy: 1.3930 - accuracy: 0.5818\n",
      "Epoch 34/50\n",
      "667/667 [==============================] - ETA: 0s - loss: 1.3925 - categorical_crossentropy: 1.3925 - accuracy: 0.5819\n",
      "----- Generating text after Epoch: 33\n",
      "667/667 [==============================] - 349s 523ms/step - loss: 1.3925 - categorical_crossentropy: 1.3925 - accuracy: 0.5819\n",
      "Epoch 35/50\n",
      "667/667 [==============================] - ETA: 0s - loss: 1.3920 - categorical_crossentropy: 1.3920 - accuracy: 0.5819\n",
      "----- Generating text after Epoch: 34\n",
      "667/667 [==============================] - 358s 537ms/step - loss: 1.3920 - categorical_crossentropy: 1.3920 - accuracy: 0.5819\n",
      "Epoch 36/50\n",
      "667/667 [==============================] - ETA: 0s - loss: 1.3914 - categorical_crossentropy: 1.3914 - accuracy: 0.5821\n",
      "----- Generating text after Epoch: 35\n",
      "667/667 [==============================] - 363s 544ms/step - loss: 1.3914 - categorical_crossentropy: 1.3914 - accuracy: 0.5821\n",
      "Epoch 37/50\n",
      "667/667 [==============================] - ETA: 0s - loss: 1.3909 - categorical_crossentropy: 1.3909 - accuracy: 0.5821\n",
      "----- Generating text after Epoch: 36\n",
      "667/667 [==============================] - 372s 558ms/step - loss: 1.3909 - categorical_crossentropy: 1.3909 - accuracy: 0.5821\n",
      "Epoch 38/50\n",
      "667/667 [==============================] - ETA: 0s - loss: 1.3904 - categorical_crossentropy: 1.3904 - accuracy: 0.5824\n",
      "----- Generating text after Epoch: 37\n",
      "667/667 [==============================] - 375s 562ms/step - loss: 1.3904 - categorical_crossentropy: 1.3904 - accuracy: 0.5824\n",
      "Epoch 39/50\n",
      "667/667 [==============================] - ETA: 0s - loss: 1.3900 - categorical_crossentropy: 1.3900 - accuracy: 0.5822\n",
      "----- Generating text after Epoch: 38\n",
      "667/667 [==============================] - 392s 588ms/step - loss: 1.3900 - categorical_crossentropy: 1.3900 - accuracy: 0.5822\n",
      "Epoch 40/50\n",
      "667/667 [==============================] - ETA: 0s - loss: 1.3897 - categorical_crossentropy: 1.3897 - accuracy: 0.5825\n",
      "----- Generating text after Epoch: 39\n",
      "667/667 [==============================] - 398s 597ms/step - loss: 1.3897 - categorical_crossentropy: 1.3897 - accuracy: 0.5825\n",
      "Epoch 41/50\n",
      "667/667 [==============================] - ETA: 0s - loss: 1.3892 - categorical_crossentropy: 1.3892 - accuracy: 0.5827\n",
      "----- Generating text after Epoch: 40\n",
      "667/667 [==============================] - 406s 609ms/step - loss: 1.3892 - categorical_crossentropy: 1.3892 - accuracy: 0.5827\n",
      "Epoch 42/50\n",
      "667/667 [==============================] - ETA: 0s - loss: 1.3887 - categorical_crossentropy: 1.3887 - accuracy: 0.5827\n",
      "----- Generating text after Epoch: 41\n",
      "667/667 [==============================] - 404s 606ms/step - loss: 1.3887 - categorical_crossentropy: 1.3887 - accuracy: 0.5827\n",
      "Epoch 43/50\n",
      "667/667 [==============================] - ETA: 0s - loss: 1.3884 - categorical_crossentropy: 1.3884 - accuracy: 0.5829\n",
      "----- Generating text after Epoch: 42\n",
      "667/667 [==============================] - 412s 618ms/step - loss: 1.3884 - categorical_crossentropy: 1.3884 - accuracy: 0.5829\n",
      "Epoch 44/50\n",
      "667/667 [==============================] - ETA: 0s - loss: 1.3880 - categorical_crossentropy: 1.3880 - accuracy: 0.5828\n",
      "----- Generating text after Epoch: 43\n",
      "667/667 [==============================] - 412s 617ms/step - loss: 1.3880 - categorical_crossentropy: 1.3880 - accuracy: 0.5828\n",
      "Epoch 45/50\n",
      "667/667 [==============================] - ETA: 0s - loss: 1.3875 - categorical_crossentropy: 1.3875 - accuracy: 0.5830\n",
      "----- Generating text after Epoch: 44\n",
      "667/667 [==============================] - 419s 629ms/step - loss: 1.3875 - categorical_crossentropy: 1.3875 - accuracy: 0.5830\n",
      "Epoch 46/50\n",
      "667/667 [==============================] - ETA: 0s - loss: 1.3873 - categorical_crossentropy: 1.3873 - accuracy: 0.5830\n",
      "----- Generating text after Epoch: 45\n",
      "667/667 [==============================] - 420s 630ms/step - loss: 1.3873 - categorical_crossentropy: 1.3873 - accuracy: 0.5830\n",
      "Epoch 47/50\n",
      "667/667 [==============================] - ETA: 0s - loss: 1.3869 - categorical_crossentropy: 1.3869 - accuracy: 0.5832\n",
      "----- Generating text after Epoch: 46\n",
      "667/667 [==============================] - 442s 663ms/step - loss: 1.3869 - categorical_crossentropy: 1.3869 - accuracy: 0.5832\n",
      "Epoch 48/50\n",
      "667/667 [==============================] - ETA: 0s - loss: 1.3867 - categorical_crossentropy: 1.3867 - accuracy: 0.5831\n",
      "----- Generating text after Epoch: 47\n",
      "667/667 [==============================] - 451s 676ms/step - loss: 1.3867 - categorical_crossentropy: 1.3867 - accuracy: 0.5831\n",
      "Epoch 49/50\n",
      "667/667 [==============================] - ETA: 0s - loss: 1.3864 - categorical_crossentropy: 1.3864 - accuracy: 0.5833\n",
      "----- Generating text after Epoch: 48\n",
      "667/667 [==============================] - 486s 728ms/step - loss: 1.3864 - categorical_crossentropy: 1.3864 - accuracy: 0.5833\n",
      "Epoch 50/50\n",
      "667/667 [==============================] - ETA: 0s - loss: 1.3858 - categorical_crossentropy: 1.3858 - accuracy: 0.5833\n",
      "----- Generating text after Epoch: 49\n",
      "667/667 [==============================] - 493s 739ms/step - loss: 1.3858 - categorical_crossentropy: 1.3858 - accuracy: 0.5833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cbe8e77340>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(text_niet)))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "seqlen = 40\n",
    "step = seqlen\n",
    "sentences = []\n",
    "for i in range(40, len(text_niet) - seqlen - 1, step):\n",
    "    sentences.append(text_niet[i: i + seqlen + 1])\n",
    "\n",
    "x = np.zeros((len(sentences), seqlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), seqlen, len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, (char_in, char_out) in enumerate(zip(sentence[:-1], sentence[1:])):\n",
    "        x[i, t, char_indices[char_in]] = 1\n",
    "        y[i, t, char_indices[char_out]] = 1\n",
    "        \n",
    "model1 = Sequential()\n",
    "model1.add(LSTM(128, input_shape=(seqlen, len(chars)), return_sequences=True))\n",
    "model1.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model1.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=RMSprop(learning_rate=0.01),\n",
    "    metrics=['categorical_crossentropy', 'accuracy']\n",
    ")\n",
    "\n",
    "def on_epoch_end_niet(epoch, _):\n",
    "    \"\"\"Function invoked at end of each epoch.\"\"\"\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text_niet) - seqlen - 1)\n",
    "\n",
    "    generated = ''\n",
    "    sentence = text_niet[start_index: start_index + seqlen]\n",
    "    generated += sentence\n",
    "    #print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    #sys.stdout.write(generated)\n",
    "\n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, seqlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_indices[char]] = 1.\n",
    "        \n",
    "        preds = model1.predict(x_pred, verbose=0)\n",
    "        next_index = sample(preds[0, -1], diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "        #sys.stdout.write(next_char) \n",
    "\n",
    "    # for diversity in [0.2, 0.5, 1.0]:\n",
    "    #     #print('----- diversity:', diversity)\n",
    "\n",
    "    #     generated = ''\n",
    "    #     sentence = text_niet[start_index: start_index + seqlen]\n",
    "    #     generated += sentence\n",
    "    #     #print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    #     #sys.stdout.write(generated)\n",
    "\n",
    "    #     for i in range(400):\n",
    "    #         x_pred = np.zeros((1, seqlen, len(chars)))\n",
    "    #         for t, char in enumerate(sentence):\n",
    "    #             x_pred[0, t, char_indices[char]] = 1.\n",
    "            \n",
    "    #         preds = model1.predict(x_pred, verbose=0)\n",
    "    #         next_index = sample(preds[0, -1], diversity)\n",
    "    #         next_char = indices_char[next_index]\n",
    "\n",
    "    #         sentence = sentence[1:] + next_char\n",
    "\n",
    "    #         #sys.stdout.write(next_char)   \n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end_niet)\n",
    "\n",
    "\n",
    "model1.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=EPOCHS,\n",
    "          callbacks=[print_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timjh\\AppData\\Local\\Temp\\ipykernel_8520\\798493350.py:11: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  x_d = np.zeros((len(sentences), seqlen, len(chars_date)), dtype=np.bool)\n",
      "C:\\Users\\timjh\\AppData\\Local\\Temp\\ipykernel_8520\\798493350.py:12: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y_d = np.zeros((len(sentences), seqlen, len(chars_date)), dtype=np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 3.5863 - categorical_crossentropy: 3.5863 - accuracy: 0.1443\n",
      "----- Generating text after Epoch: 0\n",
      "21/21 [==============================] - 38s 2s/step - loss: 3.5863 - categorical_crossentropy: 3.5863 - accuracy: 0.1443\n",
      "Epoch 2/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 3.0744 - categorical_crossentropy: 3.0744 - accuracy: 0.2218\n",
      "----- Generating text after Epoch: 1\n",
      "21/21 [==============================] - 35s 2s/step - loss: 3.0744 - categorical_crossentropy: 3.0744 - accuracy: 0.2218\n",
      "Epoch 3/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 2.5464 - categorical_crossentropy: 2.5464 - accuracy: 0.3161\n",
      "----- Generating text after Epoch: 2\n",
      "21/21 [==============================] - 36s 2s/step - loss: 2.5464 - categorical_crossentropy: 2.5464 - accuracy: 0.3161\n",
      "Epoch 4/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 2.2989 - categorical_crossentropy: 2.2989 - accuracy: 0.3622\n",
      "----- Generating text after Epoch: 3\n",
      "21/21 [==============================] - 35s 2s/step - loss: 2.2989 - categorical_crossentropy: 2.2989 - accuracy: 0.3622\n",
      "Epoch 5/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 2.1061 - categorical_crossentropy: 2.1061 - accuracy: 0.4104\n",
      "----- Generating text after Epoch: 4\n",
      "21/21 [==============================] - 36s 2s/step - loss: 2.1061 - categorical_crossentropy: 2.1061 - accuracy: 0.4104\n",
      "Epoch 6/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.9588 - categorical_crossentropy: 1.9588 - accuracy: 0.4416\n",
      "----- Generating text after Epoch: 5\n",
      "21/21 [==============================] - 36s 2s/step - loss: 1.9588 - categorical_crossentropy: 1.9588 - accuracy: 0.4416\n",
      "Epoch 7/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.8575 - categorical_crossentropy: 1.8575 - accuracy: 0.4680\n",
      "----- Generating text after Epoch: 6\n",
      "21/21 [==============================] - 36s 2s/step - loss: 1.8575 - categorical_crossentropy: 1.8575 - accuracy: 0.4680\n",
      "Epoch 8/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.7589 - categorical_crossentropy: 1.7589 - accuracy: 0.4910\n",
      "----- Generating text after Epoch: 7\n",
      "21/21 [==============================] - 37s 2s/step - loss: 1.7589 - categorical_crossentropy: 1.7589 - accuracy: 0.4910\n",
      "Epoch 9/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.6856 - categorical_crossentropy: 1.6856 - accuracy: 0.5092\n",
      "----- Generating text after Epoch: 8\n",
      "21/21 [==============================] - 36s 2s/step - loss: 1.6856 - categorical_crossentropy: 1.6856 - accuracy: 0.5092\n",
      "Epoch 10/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.6233 - categorical_crossentropy: 1.6233 - accuracy: 0.5241\n",
      "----- Generating text after Epoch: 9\n",
      "21/21 [==============================] - 37s 2s/step - loss: 1.6233 - categorical_crossentropy: 1.6233 - accuracy: 0.5241\n",
      "Epoch 11/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.5628 - categorical_crossentropy: 1.5628 - accuracy: 0.5397\n",
      "----- Generating text after Epoch: 10\n",
      "21/21 [==============================] - 37s 2s/step - loss: 1.5628 - categorical_crossentropy: 1.5628 - accuracy: 0.5397\n",
      "Epoch 12/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.5176 - categorical_crossentropy: 1.5176 - accuracy: 0.5513\n",
      "----- Generating text after Epoch: 11\n",
      "21/21 [==============================] - 37s 2s/step - loss: 1.5176 - categorical_crossentropy: 1.5176 - accuracy: 0.5513\n",
      "Epoch 13/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.4722 - categorical_crossentropy: 1.4722 - accuracy: 0.5633\n",
      "----- Generating text after Epoch: 12\n",
      "21/21 [==============================] - 37s 2s/step - loss: 1.4722 - categorical_crossentropy: 1.4722 - accuracy: 0.5633\n",
      "Epoch 14/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.4343 - categorical_crossentropy: 1.4343 - accuracy: 0.5731\n",
      "----- Generating text after Epoch: 13\n",
      "21/21 [==============================] - 37s 2s/step - loss: 1.4343 - categorical_crossentropy: 1.4343 - accuracy: 0.5731\n",
      "Epoch 15/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.3928 - categorical_crossentropy: 1.3928 - accuracy: 0.5840\n",
      "----- Generating text after Epoch: 14\n",
      "21/21 [==============================] - 37s 2s/step - loss: 1.3928 - categorical_crossentropy: 1.3928 - accuracy: 0.5840\n",
      "Epoch 16/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.3607 - categorical_crossentropy: 1.3607 - accuracy: 0.5941\n",
      "----- Generating text after Epoch: 15\n",
      "21/21 [==============================] - 37s 2s/step - loss: 1.3607 - categorical_crossentropy: 1.3607 - accuracy: 0.5941\n",
      "Epoch 17/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.3314 - categorical_crossentropy: 1.3314 - accuracy: 0.6005\n",
      "----- Generating text after Epoch: 16\n",
      "21/21 [==============================] - 38s 2s/step - loss: 1.3314 - categorical_crossentropy: 1.3314 - accuracy: 0.6005\n",
      "Epoch 18/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.2999 - categorical_crossentropy: 1.2999 - accuracy: 0.6090\n",
      "----- Generating text after Epoch: 17\n",
      "21/21 [==============================] - 38s 2s/step - loss: 1.2999 - categorical_crossentropy: 1.2999 - accuracy: 0.6090\n",
      "Epoch 19/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.2746 - categorical_crossentropy: 1.2746 - accuracy: 0.6153\n",
      "----- Generating text after Epoch: 18\n",
      "21/21 [==============================] - 38s 2s/step - loss: 1.2746 - categorical_crossentropy: 1.2746 - accuracy: 0.6153\n",
      "Epoch 20/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.2441 - categorical_crossentropy: 1.2441 - accuracy: 0.6236\n",
      "----- Generating text after Epoch: 19\n",
      "21/21 [==============================] - 38s 2s/step - loss: 1.2441 - categorical_crossentropy: 1.2441 - accuracy: 0.6236\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cc68e07730>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars_date = sorted(list(set(text_date)))\n",
    "char_indices_date = dict((c, i) for i, c in enumerate(chars_date))\n",
    "indices_char_date = dict((i, c) for i, c in enumerate(chars_date))\n",
    "\n",
    "seqlen = 40\n",
    "step = seqlen\n",
    "sentences = []\n",
    "for i in range(40, len(text_date) - seqlen - 1, step):\n",
    "    sentences.append(text_date[i: i + seqlen + 1])\n",
    "\n",
    "x_d = np.zeros((len(sentences), seqlen, len(chars_date)), dtype=np.bool)\n",
    "y_d = np.zeros((len(sentences), seqlen, len(chars_date)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, (char_in, char_out) in enumerate(zip(sentence[:-1], sentence[1:])):\n",
    "        x_d[i, t, char_indices_date[char_in]] = 1\n",
    "        y_d[i, t, char_indices_date[char_out]] = 1\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(LSTM(128, input_shape=(seqlen, len(chars_date)), return_sequences=True))\n",
    "model2.add(Dense(len(chars_date), activation='softmax'))\n",
    "\n",
    "model2.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=RMSprop(learning_rate=0.01),\n",
    "    metrics=['categorical_crossentropy', 'accuracy']\n",
    ")\n",
    "\n",
    "def on_epoch_end_date(epoch, _):\n",
    "    \"\"\"Function invoked at end of each epoch.\"\"\"\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text_date) - seqlen - 1)\n",
    "\n",
    "    generated = ''\n",
    "    sentence = text_date[start_index: start_index + seqlen]\n",
    "    generated += sentence\n",
    "    #print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    #sys.stdout.write(generated)\n",
    "\n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, seqlen, len(chars_date)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_indices_date[char]] = 1.\n",
    "        \n",
    "        preds = model2.predict(x_pred, verbose=0)\n",
    "        next_index = sample(preds[0, -1], diversity)\n",
    "        next_char = indices_char_date[next_index]\n",
    "\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end_date)\n",
    "\n",
    "model2.fit(x_d, y_d,\n",
    "          batch_size=128,\n",
    "          epochs=20,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_date = sorted(list(set(text_date)))\n",
    "char_indices_date = dict((c, i) for i, c in enumerate(chars_date))\n",
    "indices_char_date = dict((i, c) for i, c in enumerate(chars_date))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Generating with seed: \"Do we not\n",
      "hear the noise of the grave-di\"\n",
      "Do we not\n",
      "hear the noise of the grave-ding to the back to proble this not.\n",
      "LEIGH\n",
      "So, I me. This is a goom from *\n",
      "BRAD *\n",
      "Hell, tome her and\n",
      "staying each to the company.\n",
      "ALLISON *\n",
      "(reads)\n",
      "â€œPet wer. *\n",
      "Leigh and Conner and then off the end to we staof the crommmpacty.\n",
      "96 16 EXT. LEIGH AND ERINâ€™S APATT CONTINEES - DAY\n",
      "Establishing shot of the retel can.\n",
      "119111 11D ALLISONâ€™S OFFICE - DAY\n",
      "Leigh and Erin ar un on the coffee for\n",
      "them. "
     ]
    }
   ],
   "source": [
    "seqlenn = 40\n",
    "\n",
    "def generate_date(seed):\n",
    "    output = \"\"\n",
    "    print()\n",
    "    rand_evn = random.randint(0,100)\n",
    "\n",
    "    sentence = seed\n",
    "    generated = ''\n",
    "    generated += sentence\n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, seqlenn, len(chars_date)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_indices_date[char]] = 1.\n",
    "        \n",
    "        preds = model2.predict(x_pred, verbose=0)\n",
    "        next_index = sample(preds[0, -1], 0.7)\n",
    "        next_char = indices_char_date[next_index]\n",
    "\n",
    "        sentence = sentence[1:] + next_char\n",
    "        output = output + next_char\n",
    "        sys.stdout.write(next_char)\n",
    "    return output\n",
    "\n",
    "def generate_niet(seed):\n",
    "    output = \"\"\n",
    "    print()\n",
    "    sentence = seed\n",
    "    generated = ''\n",
    "\n",
    "    generated += sentence\n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    for i in range(200):\n",
    "        x_pred = np.zeros((1, seqlenn, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_indices[char]] = 1.\n",
    "        \n",
    "        preds = model1.predict(x_pred, verbose=0)\n",
    "        next_index = sample(preds[0, -1], 0.7)\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        sentence = sentence[1:] + next_char\n",
    "        output = output + next_char\n",
    "        sys.stdout.write(next_char)\n",
    "    return output\n",
    "\n",
    "output = text_niet[732020:732176]\n",
    "\n",
    "for i in range(0,10):\n",
    "    if i % 2 == 0:\n",
    "        print(\"\\nGenerating Hallmark Text\\n\")\n",
    "        output = generate_date(output[:40])\n",
    "    else:\n",
    "        print(\"\\nGenerating Nietzsche Text\\n\")\n",
    "        output = generate_niet(output[:40])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
