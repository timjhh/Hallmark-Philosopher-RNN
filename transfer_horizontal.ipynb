{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import requests as rq\n",
    "import sys\n",
    "import io\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import keras\n",
    "import keras.callbacks\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"nietzsche.txt\", encoding=\"utf-8\")\n",
    "text_niet = file.read()\n",
    "# 732020:732176\n",
    "\n",
    "file_date = open(\"baddate.txt\", encoding=\"utf-8\")\n",
    "text_date = file_date.read()\n",
    "\n",
    "EPOCHS = 10\n",
    "diversity = 0.5\n",
    "\n",
    "# Do we not\n",
    "# hear the noise of the grave-diggers who are burying God? Do we not smell\n",
    "# the divine putrefaction?â€”for even Gods putrefy! God is dead! God remains\n",
    "# dead! And we have killed him!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    \"\"\"Helper function to sample an index from a probability array.\"\"\"\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.exp(np.log(preds) / temperature)\n",
    "    preds = preds / np.sum(preds)               \n",
    "    probas = np.random.multinomial(1, preds, 1) \n",
    "    return np.argmax(probas)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timjh\\AppData\\Local\\Temp\\ipykernel_1696\\1356494578.py:11: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  x = np.zeros((len(sentences), seqlen, len(chars)), dtype=np.bool)\n",
      "C:\\Users\\timjh\\AppData\\Local\\Temp\\ipykernel_1696\\1356494578.py:12: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y = np.zeros((len(sentences), seqlen, len(chars)), dtype=np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "667/667 [==============================] - ETA: 0s - loss: 1.9682 - categorical_crossentropy: 1.9682 - accuracy: 0.4375\n",
      "----- Generating text after Epoch: 0\n",
      "667/667 [==============================] - 100s 147ms/step - loss: 1.9682 - categorical_crossentropy: 1.9682 - accuracy: 0.4375\n",
      "Epoch 2/10\n",
      "666/667 [============================>.] - ETA: 0s - loss: 1.5685 - categorical_crossentropy: 1.5685 - accuracy: 0.5384\n",
      "----- Generating text after Epoch: 1\n",
      "667/667 [==============================] - 95s 142ms/step - loss: 1.5684 - categorical_crossentropy: 1.5684 - accuracy: 0.5384\n",
      "Epoch 3/10\n",
      "666/667 [============================>.] - ETA: 0s - loss: 1.5072 - categorical_crossentropy: 1.5072 - accuracy: 0.5540\n",
      "----- Generating text after Epoch: 2\n",
      "667/667 [==============================] - 92s 138ms/step - loss: 1.5072 - categorical_crossentropy: 1.5072 - accuracy: 0.5540\n",
      "Epoch 4/10\n",
      "666/667 [============================>.] - ETA: 0s - loss: 1.4794 - categorical_crossentropy: 1.4794 - accuracy: 0.5608\n",
      "----- Generating text after Epoch: 3\n",
      "667/667 [==============================] - 89s 133ms/step - loss: 1.4795 - categorical_crossentropy: 1.4795 - accuracy: 0.5608\n",
      "Epoch 5/10\n",
      "666/667 [============================>.] - ETA: 0s - loss: 1.4628 - categorical_crossentropy: 1.4628 - accuracy: 0.5650\n",
      "----- Generating text after Epoch: 4\n",
      "667/667 [==============================] - 88s 133ms/step - loss: 1.4628 - categorical_crossentropy: 1.4628 - accuracy: 0.5650\n",
      "Epoch 6/10\n",
      "666/667 [============================>.] - ETA: 0s - loss: 1.4511 - categorical_crossentropy: 1.4511 - accuracy: 0.5678\n",
      "----- Generating text after Epoch: 5\n",
      "667/667 [==============================] - 105s 158ms/step - loss: 1.4511 - categorical_crossentropy: 1.4511 - accuracy: 0.5678\n",
      "Epoch 7/10\n",
      "666/667 [============================>.] - ETA: 0s - loss: 1.4426 - categorical_crossentropy: 1.4426 - accuracy: 0.5697\n",
      "----- Generating text after Epoch: 6\n",
      "667/667 [==============================] - 89s 133ms/step - loss: 1.4426 - categorical_crossentropy: 1.4426 - accuracy: 0.5697\n",
      "Epoch 8/10\n",
      "666/667 [============================>.] - ETA: 0s - loss: 1.4361 - categorical_crossentropy: 1.4361 - accuracy: 0.5714\n",
      "----- Generating text after Epoch: 7\n",
      "667/667 [==============================] - 81s 122ms/step - loss: 1.4362 - categorical_crossentropy: 1.4362 - accuracy: 0.5714\n",
      "Epoch 9/10\n",
      "666/667 [============================>.] - ETA: 0s - loss: 1.4309 - categorical_crossentropy: 1.4309 - accuracy: 0.5724\n",
      "----- Generating text after Epoch: 8\n",
      "667/667 [==============================] - 91s 137ms/step - loss: 1.4309 - categorical_crossentropy: 1.4309 - accuracy: 0.5724\n",
      "Epoch 10/10\n",
      "666/667 [============================>.] - ETA: 0s - loss: 1.4266 - categorical_crossentropy: 1.4266 - accuracy: 0.5736\n",
      "----- Generating text after Epoch: 9\n",
      "667/667 [==============================] - 91s 137ms/step - loss: 1.4266 - categorical_crossentropy: 1.4266 - accuracy: 0.5736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ff9bd13ee0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(text_niet)))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "seqlen = 40\n",
    "step = seqlen\n",
    "sentences = []\n",
    "for i in range(40, len(text_niet) - seqlen - 1, step):\n",
    "    sentences.append(text_niet[i: i + seqlen + 1])\n",
    "\n",
    "x = np.zeros((len(sentences), seqlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), seqlen, len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, (char_in, char_out) in enumerate(zip(sentence[:-1], sentence[1:])):\n",
    "        x[i, t, char_indices[char_in]] = 1\n",
    "        y[i, t, char_indices[char_out]] = 1\n",
    "        \n",
    "model1 = Sequential()\n",
    "model1.add(LSTM(128, input_shape=(seqlen, len(chars)), return_sequences=True))\n",
    "model1.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model1.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=RMSprop(learning_rate=0.01),\n",
    "    metrics=['categorical_crossentropy', 'accuracy']\n",
    ")\n",
    "\n",
    "def on_epoch_end_niet(epoch, _):\n",
    "    \"\"\"Function invoked at end of each epoch.\"\"\"\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text_niet) - seqlen - 1)\n",
    "\n",
    "    generated = ''\n",
    "    sentence = text_niet[start_index: start_index + seqlen]\n",
    "    generated += sentence\n",
    "    #print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    #sys.stdout.write(generated)\n",
    "\n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, seqlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_indices[char]] = 1.\n",
    "        \n",
    "        preds = model1.predict(x_pred, verbose=0)\n",
    "        next_index = sample(preds[0, -1], diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "        #sys.stdout.write(next_char) \n",
    "\n",
    "    # for diversity in [0.2, 0.5, 1.0]:\n",
    "    #     #print('----- diversity:', diversity)\n",
    "\n",
    "    #     generated = ''\n",
    "    #     sentence = text_niet[start_index: start_index + seqlen]\n",
    "    #     generated += sentence\n",
    "    #     #print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    #     #sys.stdout.write(generated)\n",
    "\n",
    "    #     for i in range(400):\n",
    "    #         x_pred = np.zeros((1, seqlen, len(chars)))\n",
    "    #         for t, char in enumerate(sentence):\n",
    "    #             x_pred[0, t, char_indices[char]] = 1.\n",
    "            \n",
    "    #         preds = model1.predict(x_pred, verbose=0)\n",
    "    #         next_index = sample(preds[0, -1], diversity)\n",
    "    #         next_char = indices_char[next_index]\n",
    "\n",
    "    #         sentence = sentence[1:] + next_char\n",
    "\n",
    "    #         #sys.stdout.write(next_char)   \n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end_niet)\n",
    "\n",
    "\n",
    "model1.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=EPOCHS,\n",
    "          callbacks=[print_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timjh\\AppData\\Local\\Temp\\ipykernel_1696\\752095980.py:11: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  x_d = np.zeros((len(sentences), seqlen, len(chars_date)), dtype=np.bool)\n",
      "C:\\Users\\timjh\\AppData\\Local\\Temp\\ipykernel_1696\\752095980.py:12: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y_d = np.zeros((len(sentences), seqlen, len(chars_date)), dtype=np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 3.6126 - categorical_crossentropy: 3.6126 - accuracy: 0.1428\n",
      "----- Generating text after Epoch: 0\n",
      "21/21 [==============================] - 25s 1s/step - loss: 3.6080 - categorical_crossentropy: 3.6080 - accuracy: 0.1431\n",
      "Epoch 2/10\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 3.2789 - categorical_crossentropy: 3.2789 - accuracy: 0.1816\n",
      "----- Generating text after Epoch: 1\n",
      "21/21 [==============================] - 23s 1s/step - loss: 3.2746 - categorical_crossentropy: 3.2746 - accuracy: 0.1822\n",
      "Epoch 3/10\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 2.7622 - categorical_crossentropy: 2.7622 - accuracy: 0.2686\n",
      "----- Generating text after Epoch: 2\n",
      "21/21 [==============================] - 22s 1s/step - loss: 2.7591 - categorical_crossentropy: 2.7591 - accuracy: 0.2691\n",
      "Epoch 4/10\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 2.4677 - categorical_crossentropy: 2.4677 - accuracy: 0.3227\n",
      "----- Generating text after Epoch: 3\n",
      "21/21 [==============================] - 22s 1s/step - loss: 2.4642 - categorical_crossentropy: 2.4642 - accuracy: 0.3238\n",
      "Epoch 5/10\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 2.2540 - categorical_crossentropy: 2.2540 - accuracy: 0.3776\n",
      "----- Generating text after Epoch: 4\n",
      "21/21 [==============================] - 22s 1s/step - loss: 2.2532 - categorical_crossentropy: 2.2532 - accuracy: 0.3776\n",
      "Epoch 6/10\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 2.0692 - categorical_crossentropy: 2.0692 - accuracy: 0.4185\n",
      "----- Generating text after Epoch: 5\n",
      "21/21 [==============================] - 23s 1s/step - loss: 2.0699 - categorical_crossentropy: 2.0699 - accuracy: 0.4182\n",
      "Epoch 7/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.9595 - categorical_crossentropy: 1.9595 - accuracy: 0.4437\n",
      "----- Generating text after Epoch: 6\n",
      "21/21 [==============================] - 24s 1s/step - loss: 1.9595 - categorical_crossentropy: 1.9595 - accuracy: 0.4437\n",
      "Epoch 8/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.8535 - categorical_crossentropy: 1.8535 - accuracy: 0.4656\n",
      "----- Generating text after Epoch: 7\n",
      "21/21 [==============================] - 25s 1s/step - loss: 1.8535 - categorical_crossentropy: 1.8535 - accuracy: 0.4656\n",
      "Epoch 9/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.7673 - categorical_crossentropy: 1.7673 - accuracy: 0.4876\n",
      "----- Generating text after Epoch: 8\n",
      "21/21 [==============================] - 25s 1s/step - loss: 1.7673 - categorical_crossentropy: 1.7673 - accuracy: 0.4876\n",
      "Epoch 10/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.6979 - categorical_crossentropy: 1.6979 - accuracy: 0.5041\n",
      "----- Generating text after Epoch: 9\n",
      "21/21 [==============================] - 26s 1s/step - loss: 1.6979 - categorical_crossentropy: 1.6979 - accuracy: 0.5041\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ff9cb73340>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars_date = sorted(list(set(text_date)))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars_date))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars_date))\n",
    "\n",
    "seqlen = 40\n",
    "step = seqlen\n",
    "sentences = []\n",
    "for i in range(40, len(text_date) - seqlen - 1, step):\n",
    "    sentences.append(text_date[i: i + seqlen + 1])\n",
    "\n",
    "x_d = np.zeros((len(sentences), seqlen, len(chars_date)), dtype=np.bool)\n",
    "y_d = np.zeros((len(sentences), seqlen, len(chars_date)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, (char_in, char_out) in enumerate(zip(sentence[:-1], sentence[1:])):\n",
    "        x_d[i, t, char_indices[char_in]] = 1\n",
    "        y_d[i, t, char_indices[char_out]] = 1\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(LSTM(128, input_shape=(seqlen, len(chars_date)), return_sequences=True))\n",
    "model2.add(Dense(len(chars_date), activation='softmax'))\n",
    "\n",
    "model2.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=RMSprop(learning_rate=0.01),\n",
    "    metrics=['categorical_crossentropy', 'accuracy']\n",
    ")\n",
    "\n",
    "def on_epoch_end_date(epoch, _):\n",
    "    \"\"\"Function invoked at end of each epoch.\"\"\"\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text_date) - seqlen - 1)\n",
    "\n",
    "    generated = ''\n",
    "    sentence = text_date[start_index: start_index + seqlen]\n",
    "    generated += sentence\n",
    "    #print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    #sys.stdout.write(generated)\n",
    "\n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, seqlen, len(chars_date)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_indices[char]] = 1.\n",
    "        \n",
    "        preds = model2.predict(x_pred, verbose=0)\n",
    "        next_index = sample(preds[0, -1], diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end_date)\n",
    "\n",
    "model2.fit(x_d, y_d,\n",
    "          batch_size=128,\n",
    "          epochs=EPOCHS,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(char_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"lstm_14\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 6480)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [39], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m flat1 \u001b[39m=\u001b[39m Flatten()(model1\u001b[39m.\u001b[39mlayers[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39moutput)\n\u001b[0;32m      2\u001b[0m flat2 \u001b[39m=\u001b[39m Flatten()(model2\u001b[39m.\u001b[39mlayers[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39moutput)\n\u001b[1;32m----> 3\u001b[0m output \u001b[39m=\u001b[39m LSTM(\u001b[39m128\u001b[39m, input_shape\u001b[39m=\u001b[39m(seqlen, \u001b[39mlen\u001b[39m(chars)), return_sequences\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(flat1)\n\u001b[0;32m      5\u001b[0m model3 \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mModel(inputs\u001b[39m=\u001b[39m[model1\u001b[39m.\u001b[39minputs,model2\u001b[39m.\u001b[39minputs], outputs\u001b[39m=\u001b[39moutput)\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(model3\u001b[39m.\u001b[39mlayers)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\keras\\layers\\rnn\\base_rnn.py:553\u001b[0m, in \u001b[0;36mRNN.__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m inputs, initial_state, constants \u001b[39m=\u001b[39m rnn_utils\u001b[39m.\u001b[39mstandardize_args(\n\u001b[0;32m    549\u001b[0m     inputs, initial_state, constants, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_constants\n\u001b[0;32m    550\u001b[0m )\n\u001b[0;32m    552\u001b[0m \u001b[39mif\u001b[39;00m initial_state \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m constants \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    555\u001b[0m \u001b[39m# If any of `initial_state` or `constants` are specified and are Keras\u001b[39;00m\n\u001b[0;32m    556\u001b[0m \u001b[39m# tensors, then add them to the inputs and temporarily modify the\u001b[39;00m\n\u001b[0;32m    557\u001b[0m \u001b[39m# input_spec to include them.\u001b[39;00m\n\u001b[0;32m    559\u001b[0m additional_inputs \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\keras\\engine\\input_spec.py:232\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    230\u001b[0m     ndim \u001b[39m=\u001b[39m shape\u001b[39m.\u001b[39mrank\n\u001b[0;32m    231\u001b[0m     \u001b[39mif\u001b[39;00m ndim \u001b[39m!=\u001b[39m spec\u001b[39m.\u001b[39mndim:\n\u001b[1;32m--> 232\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    233\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00minput_index\u001b[39m}\u001b[39;00m\u001b[39m of layer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlayer_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    234\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mis incompatible with the layer: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    235\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpected ndim=\u001b[39m\u001b[39m{\u001b[39;00mspec\u001b[39m.\u001b[39mndim\u001b[39m}\u001b[39;00m\u001b[39m, found ndim=\u001b[39m\u001b[39m{\u001b[39;00mndim\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    236\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFull shape received: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(shape)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m         )\n\u001b[0;32m    238\u001b[0m \u001b[39mif\u001b[39;00m spec\u001b[39m.\u001b[39mmax_ndim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    239\u001b[0m     ndim \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\u001b[39m.\u001b[39mrank\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"lstm_14\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 6480)"
     ]
    }
   ],
   "source": [
    "flat1 = Flatten()(model1.layers[-1].output)\n",
    "flat2 = Flatten()(model2.layers[-1].output)\n",
    "output = LSTM(128, input_shape=(seqlen, len(chars)), return_sequences=True)(flat1)\n",
    "\n",
    "model3 = keras.Model(inputs=[model1.inputs,model2.inputs], outputs=output)\n",
    "\n",
    "for i in range(len(model3.layers)-1):\n",
    "    layer = model3.layers[i]\n",
    "    layer.trainable = False\n",
    "\n",
    "model3.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    \"\"\"Function invoked at end of each epoch.\"\"\"\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    rand_evn = random.randint(0,100)\n",
    "\n",
    "    generated = ''\n",
    "    if rand_evn % 2 == 0:\n",
    "        start_index = random.randint(0, len(text_date) - seqlen - 1)\n",
    "        sentence = text_date[start_index: start_index + seqlen]\n",
    "    else:\n",
    "        start_index = random.randint(0, len(text_niet) - seqlen - 1)\n",
    "        sentence = text_niet[start_index: start_index + seqlen]\n",
    "    generated += sentence\n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, seqlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_indices[char]] = 1.\n",
    "        \n",
    "        preds = model3.predict(x_pred, verbose=0)\n",
    "        next_index = sample(preds[0, -1], 0.5)\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    " \n",
    "model3.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=EPOCHS,\n",
    "          callbacks=[print_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
