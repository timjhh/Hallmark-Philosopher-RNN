{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import requests as rq\n",
    "import sys\n",
    "import io\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import keras\n",
    "import keras.callbacks\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE BAD DATE CHRONICLES\n",
      "Written by\n",
      "Rick Garman\n",
      "Based on a story by\n",
      "Jennifer Notas\n",
      "PRODUCTION WHITE: 3/21/16\n",
      "BLUE TBD\n",
      "Bad Date Productions\n",
      "2400 Boundary Road\n",
      "Burnaby, BC, V5M 3Z3\n",
      "Ph: 604.292.5260\n",
      "Fax: 604.628.3001\n",
      "1 1 EXT. PORTLAND - NIGHT\n",
      "Establishing shot of Portland, Oregon. *\n",
      "2 2 EXT. RICHARDS RE\n"
     ]
    }
   ],
   "source": [
    "file = open(\"baddate.txt\", encoding=\"utf-8\")\n",
    "text = file.read()\n",
    "\n",
    "print(text[0:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\syosh\\AppData\\Local\\Temp\\ipykernel_23696\\1048562009.py:12: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  x = np.zeros((len(sentences), seqlen, len(chars)), dtype=np.bool)\n",
      "C:\\Users\\syosh\\AppData\\Local\\Temp\\ipykernel_23696\\1048562009.py:13: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y = np.zeros((len(sentences), seqlen, len(chars)), dtype=np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "22/22 [==============================] - ETA: 0s - loss: 3.5813 - categorical_crossentropy: 3.5813 - accuracy: 0.1403\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ew.\n",
      "It’ll be huge!\n",
      "QUICK CUT TO:\n",
      "47 47 I\"\n",
      "ew.\n",
      "It’ll be huge!\n",
      "QUICK CUT TO:\n",
      "47 47 Iree a i  ital  e as t lt e m  e e ila arn i aare ie a t  e t aneee a\n",
      " o a \n",
      "oe\n",
      "ce et \n",
      " we  ea h es u e    al  hhe t e  ahee h a he  i iaIshareti    e    re\n",
      "e li e  t a l  n oH r u t e h    aor  t o.\n",
      "n st ane  a i l lo tu e. he a  o. au   at t ngir \n",
      " I\n",
      "22/22 [==============================] - 40s 2s/step - loss: 3.5813 - categorical_crossentropy: 3.5813 - accuracy: 0.1403are . t nthanewatore ae  ee\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - ETA: 0s - loss: 2.9567 - categorical_crossentropy: 2.9567 - accuracy: 0.2396\n",
      "----- Generating text after Epoch: 1\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"e you\n",
      "don’t want me to take you home?\n",
      "LE\"\n",
      "e you\n",
      "don’t want me to take you home?\n",
      "LEIGH\n",
      "(AD ERISONTT\n",
      "A(D\n",
      "ERNGH Than \n",
      "RESON\n",
      "The CON\n",
      "Thi Thing ans *\n",
      "SONEto O AERINERDEReD\n",
      "ALERASSONNEREIGH\n",
      "ER *\n",
      "CONNEIGH\n",
      "AERNNTRISNARD\n",
      "(RINEREISONERER’S AEREIGH\n",
      "ER\n",
      "Than Rous ou hou he tho thon .\n",
      "LEIGH\n",
      "AEIGH\n",
      "ERISONNTONER\n",
      "ODNERESENERSHTONREREIGH Oh T han he wan  oum foe.\n",
      "*\n",
      "LEIGH\n",
      "(ER D aou a ane thily *\n",
      "LILONNTERERAD SENTERDER\n",
      "ONER\n",
      "D\n",
      "ERRSONT AD\n",
      "ER T D ERISON\n",
      "22/22 [==============================] - 37s 2s/step - loss: 2.9567 - categorical_crossentropy: 2.9567 - accuracy: 0.2396\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - ETA: 0s - loss: 2.4764 - categorical_crossentropy: 2.4764 - accuracy: 0.3270\n",
      "----- Generating text after Epoch: 2\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ol\n",
      "stuff the president does like the\n",
      "car\"\n",
      "ol\n",
      "stuff the president does like the\n",
      "car thadd thang or  of re thes thes and ant the and on me thes  an tond the the this tore. *\n",
      "Les the walk thevere bees yes ous tang. she ghes the che there tong we to thes at st los you ta he ther ang the thet and se dor to. *\n",
      "LI me wat thing thes ses the yous thed on ting thes ther thes thing thes taing to then thend the thect besiste the thecgente thesn the ghate thing and\n",
      "22/22 [==============================] - 36s 2s/step - loss: 2.4764 - categorical_crossentropy: 2.4764 - accuracy: 0.3270\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - ETA: 0s - loss: 2.2100 - categorical_crossentropy: 2.2100 - accuracy: 0.3904\n",
      "----- Generating text after Epoch: 3\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"rung across the top.\n",
      "Conner stops when h\"\n",
      "rung across the top.\n",
      "Conner stops when har.\n",
      "BRAD) ALLISON\n",
      "Whas she the bay war belte he thes to the sis as and the lo the sto the pone she the the lit he the tond the she fit the dad the was care wale to the loat that sit the cat the low. Whe the the but he day.\n",
      "LEIGH\n",
      "ONNER\n",
      "22/22 [==============================] - 37s 2s/step - loss: 2.2100 - categorical_crossentropy: 2.2100 - accuracy: 0.3904 at the the tall. I’m so the hap the she st\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - ETA: 0s - loss: 2.0390 - categorical_crossentropy: 2.0390 - accuracy: 0.4252\n",
      "----- Generating text after Epoch: 4\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" Leigh and Conner *\n",
      "are having dinner. *\"\n",
      " Leigh and Conner *\n",
      "are having dinner. *\n",
      "So Rev. LEIGH (CONT’D)\n",
      "Blut and shat ste stat’s and wath the at heve *\n",
      "the wans on the to the son the sut it wast the shand to bath the pret and just reat to go the and bething the as and with in on a dads at it at in of it that so the tad ay a dith and that and do no he the cand and to beth was ablist st I’t hat and to hat in thes table so that wat ther.\n",
      "ALLISON *\n",
      "LEIGH\n",
      "22/22 [==============================] - 36s 2s/step - loss: 2.0390 - categorical_crossentropy: 2.0390 - accuracy: 0.4252\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.9075 - categorical_crossentropy: 1.9075 - accuracy: 0.4550\n",
      "----- Generating text after Epoch: 5\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"SingleSurfer87.\n",
      "PS. I’m a fan of food, t\"\n",
      "SingleSurfer87.\n",
      "PS. I’m a fan of food, to and to bean seour bather and the shat atare it out\n",
      "and to to to she wes to going to somes of to to buts in the wast.\n",
      "Blue Rev. (03/28/16) 64.\n",
      "LEIGH\n",
      "You’se watr on to thing sisus and and here. *\n",
      "LEIGH\n",
      "West in the to bout the somert of to to the ssot is in the hat of to here to the cand with hither and to got the to lith the bach.\n",
      "LEIGH\n",
      "22/22 [==============================] - 37s 2s/step - loss: 1.9075 - categorical_crossentropy: 1.9075 - accuracy: 0.4550\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.8007 - categorical_crossentropy: 1.8007 - accuracy: 0.4806\n",
      "----- Generating text after Epoch: 6\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" aloud as she types)\n",
      "(MORE)\n",
      "Blue Rev. (0\"\n",
      " aloud as she types)\n",
      "(MORE)\n",
      "Blue Rev. (03/28/16) 100.\n",
      "ALLISON\n",
      "What hroner.\n",
      "ALLISON\n",
      "What have the bad the is is thing and Erin the canding the shind the stey the gaing that the beat and the shing for been\n",
      "is and the  appare b"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\syosh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2693\u001b[0m, in \u001b[0;36mOperation.get_attr\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2692\u001b[0m \u001b[39mwith\u001b[39;00m c_api_util\u001b[39m.\u001b[39mtf_buffer() \u001b[39mas\u001b[39;00m buf:\n\u001b[1;32m-> 2693\u001b[0m   pywrap_tf_session\u001b[39m.\u001b[39;49mTF_OperationGetAttrValueProto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_c_op, name, buf)\n\u001b[0;32m   2694\u001b[0m   data \u001b[39m=\u001b[39m pywrap_tf_session\u001b[39m.\u001b[39mTF_GetBuffer(buf)\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Operation 'Slice/size' has no attr named '_read_only_resource_inputs'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\syosh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\auto_control_deps_utils.py:105\u001b[0m, in \u001b[0;36mget_read_write_resource_inputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m   read_only_input_indices \u001b[39m=\u001b[39m op\u001b[39m.\u001b[39;49mget_attr(READ_ONLY_RESOURCE_INPUTS_ATTR)\n\u001b[0;32m    106\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m    107\u001b[0m   \u001b[39m# Attr was not set. Add all resource inputs to `writes` and return.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\syosh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2697\u001b[0m, in \u001b[0;36mOperation.get_attr\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2695\u001b[0m \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mInvalidArgumentError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   2696\u001b[0m   \u001b[39m# Convert to ValueError for backwards compatibility.\u001b[39;00m\n\u001b[1;32m-> 2697\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(e\u001b[39m.\u001b[39mmessage)\n\u001b[0;32m   2698\u001b[0m x \u001b[39m=\u001b[39m attr_value_pb2\u001b[39m.\u001b[39mAttrValue()\n",
      "\u001b[1;31mValueError\u001b[0m: Operation 'Slice/size' has no attr named '_read_only_resource_inputs'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\syosh\\OneDrive\\ドキュメント\\GitHub\\Hallmark-Philosopher-RNN\\transfer_vertical.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 71>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/syosh/OneDrive/%E3%83%89%E3%82%AD%E3%83%A5%E3%83%A1%E3%83%B3%E3%83%88/GitHub/Hallmark-Philosopher-RNN/transfer_vertical.ipynb#W2sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m             sys\u001b[39m.\u001b[39mstdout\u001b[39m.\u001b[39mwrite(next_char)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/syosh/OneDrive/%E3%83%89%E3%82%AD%E3%83%A5%E3%83%A1%E3%83%B3%E3%83%88/GitHub/Hallmark-Philosopher-RNN/transfer_vertical.ipynb#W2sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m print_callback \u001b[39m=\u001b[39m LambdaCallback(on_epoch_end\u001b[39m=\u001b[39mon_epoch_end)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/syosh/OneDrive/%E3%83%89%E3%82%AD%E3%83%A5%E3%83%A1%E3%83%B3%E3%83%88/GitHub/Hallmark-Philosopher-RNN/transfer_vertical.ipynb#W2sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(x, y,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/syosh/OneDrive/%E3%83%89%E3%82%AD%E3%83%A5%E3%83%A1%E3%83%B3%E3%83%88/GitHub/Hallmark-Philosopher-RNN/transfer_vertical.ipynb#W2sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m           batch_size\u001b[39m=\u001b[39;49m\u001b[39m120\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/syosh/OneDrive/%E3%83%89%E3%82%AD%E3%83%A5%E3%83%A1%E3%83%B3%E3%83%88/GitHub/Hallmark-Philosopher-RNN/transfer_vertical.ipynb#W2sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m           epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/syosh/OneDrive/%E3%83%89%E3%82%AD%E3%83%A5%E3%83%A1%E3%83%B3%E3%83%88/GitHub/Hallmark-Philosopher-RNN/transfer_vertical.ipynb#W2sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m           callbacks\u001b[39m=\u001b[39;49m[print_callback])\n",
      "File \u001b[1;32mc:\\Users\\syosh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\syosh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1624\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1619\u001b[0m     val_logs \u001b[39m=\u001b[39m {\n\u001b[0;32m   1620\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mval_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name: val \u001b[39mfor\u001b[39;00m name, val \u001b[39min\u001b[39;00m val_logs\u001b[39m.\u001b[39mitems()\n\u001b[0;32m   1621\u001b[0m     }\n\u001b[0;32m   1622\u001b[0m     epoch_logs\u001b[39m.\u001b[39mupdate(val_logs)\n\u001b[1;32m-> 1624\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_epoch_end(epoch, epoch_logs)\n\u001b[0;32m   1625\u001b[0m training_logs \u001b[39m=\u001b[39m epoch_logs\n\u001b[0;32m   1626\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mc:\\Users\\syosh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:448\u001b[0m, in \u001b[0;36mCallbackList.on_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    446\u001b[0m logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_logs(logs)\n\u001b[0;32m    447\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[1;32m--> 448\u001b[0m     callback\u001b[39m.\u001b[39;49mon_epoch_end(epoch, logs)\n",
      "\u001b[1;32mc:\\Users\\syosh\\OneDrive\\ドキュメント\\GitHub\\Hallmark-Philosopher-RNN\\transfer_vertical.ipynb Cell 3\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(epoch, _)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/syosh/OneDrive/%E3%83%89%E3%82%AD%E3%83%A5%E3%83%A1%E3%83%B3%E3%83%88/GitHub/Hallmark-Philosopher-RNN/transfer_vertical.ipynb#W2sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mfor\u001b[39;00m t, char \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(sentence):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/syosh/OneDrive/%E3%83%89%E3%82%AD%E3%83%A5%E3%83%A1%E3%83%B3%E3%83%88/GitHub/Hallmark-Philosopher-RNN/transfer_vertical.ipynb#W2sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     x_pred[\u001b[39m0\u001b[39m, t, char_indices[char]] \u001b[39m=\u001b[39m \u001b[39m1.\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/syosh/OneDrive/%E3%83%89%E3%82%AD%E3%83%A5%E3%83%A1%E3%83%B3%E3%83%88/GitHub/Hallmark-Philosopher-RNN/transfer_vertical.ipynb#W2sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m preds \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(x_pred, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/syosh/OneDrive/%E3%83%89%E3%82%AD%E3%83%A5%E3%83%A1%E3%83%B3%E3%83%88/GitHub/Hallmark-Philosopher-RNN/transfer_vertical.ipynb#W2sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m next_index \u001b[39m=\u001b[39m sample(preds[\u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], diversity)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/syosh/OneDrive/%E3%83%89%E3%82%AD%E3%83%A5%E3%83%A1%E3%83%B3%E3%83%88/GitHub/Hallmark-Philosopher-RNN/transfer_vertical.ipynb#W2sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m next_char \u001b[39m=\u001b[39m indices_char[next_index]\n",
      "File \u001b[1;32mc:\\Users\\syosh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\syosh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:2220\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2211\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m   2212\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   2213\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUsing Model.predict with MultiWorkerMirroredStrategy \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2214\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mor TPUStrategy and AutoShardPolicy.FILE might lead to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2217\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m   2218\u001b[0m         )\n\u001b[1;32m-> 2220\u001b[0m data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49mget_data_handler(\n\u001b[0;32m   2221\u001b[0m     x\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m   2222\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   2223\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps,\n\u001b[0;32m   2224\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m   2225\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m   2226\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   2227\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   2228\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   2229\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   2230\u001b[0m     steps_per_execution\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution,\n\u001b[0;32m   2231\u001b[0m )\n\u001b[0;32m   2233\u001b[0m \u001b[39m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   2234\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(callbacks, callbacks_module\u001b[39m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32mc:\\Users\\syosh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\data_adapter.py:1582\u001b[0m, in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1580\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(kwargs[\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39m_cluster_coordinator\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1581\u001b[0m     \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> 1582\u001b[0m \u001b[39mreturn\u001b[39;00m DataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\syosh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\data_adapter.py:1262\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1259\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution \u001b[39m=\u001b[39m steps_per_execution\n\u001b[0;32m   1261\u001b[0m adapter_cls \u001b[39m=\u001b[39m select_data_adapter(x, y)\n\u001b[1;32m-> 1262\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter \u001b[39m=\u001b[39m adapter_cls(\n\u001b[0;32m   1263\u001b[0m     x,\n\u001b[0;32m   1264\u001b[0m     y,\n\u001b[0;32m   1265\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   1266\u001b[0m     steps\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[0;32m   1267\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs \u001b[39m-\u001b[39;49m initial_epoch,\n\u001b[0;32m   1268\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1269\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   1270\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1271\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1272\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1273\u001b[0m     distribution_strategy\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mdistribute\u001b[39m.\u001b[39;49mget_strategy(),\n\u001b[0;32m   1274\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m   1275\u001b[0m )\n\u001b[0;32m   1277\u001b[0m strategy \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mget_strategy()\n\u001b[0;32m   1279\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\syosh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\data_adapter.py:347\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    344\u001b[0m         flat_dataset \u001b[39m=\u001b[39m flat_dataset\u001b[39m.\u001b[39mshuffle(\u001b[39m1024\u001b[39m)\u001b[39m.\u001b[39mrepeat(epochs)\n\u001b[0;32m    345\u001b[0m     \u001b[39mreturn\u001b[39;00m flat_dataset\n\u001b[1;32m--> 347\u001b[0m indices_dataset \u001b[39m=\u001b[39m indices_dataset\u001b[39m.\u001b[39;49mflat_map(slice_batch_indices)\n\u001b[0;32m    349\u001b[0m dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslice_inputs(indices_dataset, inputs)\n\u001b[0;32m    351\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\syosh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:2245\u001b[0m, in \u001b[0;36mDatasetV2.flat_map\u001b[1;34m(self, map_func, name)\u001b[0m\n\u001b[0;32m   2212\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflat_map\u001b[39m(\u001b[39mself\u001b[39m, map_func, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   2213\u001b[0m   \u001b[39m\"\"\"Maps `map_func` across this dataset and flattens the result.\u001b[39;00m\n\u001b[0;32m   2214\u001b[0m \n\u001b[0;32m   2215\u001b[0m \u001b[39m  The type signature is:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2243\u001b[0m \u001b[39m    Dataset: A `Dataset`.\u001b[39;00m\n\u001b[0;32m   2244\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2245\u001b[0m   \u001b[39mreturn\u001b[39;00m FlatMapDataset(\u001b[39mself\u001b[39;49m, map_func, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\syosh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:5484\u001b[0m, in \u001b[0;36mFlatMapDataset.__init__\u001b[1;34m(self, input_dataset, map_func, name)\u001b[0m\n\u001b[0;32m   5482\u001b[0m \u001b[39m\"\"\"See `Dataset.flat_map()` for details.\"\"\"\u001b[39;00m\n\u001b[0;32m   5483\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_dataset \u001b[39m=\u001b[39m input_dataset\n\u001b[1;32m-> 5484\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func \u001b[39m=\u001b[39m structured_function\u001b[39m.\u001b[39;49mStructuredFunctionWrapper(\n\u001b[0;32m   5485\u001b[0m     map_func, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transformation_name(), dataset\u001b[39m=\u001b[39;49minput_dataset)\n\u001b[0;32m   5486\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func\u001b[39m.\u001b[39moutput_structure, DatasetSpec):\n\u001b[0;32m   5487\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   5488\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mThe `map_func` argument must return a `Dataset` object. Got \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   5489\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m_get_type(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func\u001b[39m.\u001b[39moutput_structure)\u001b[39m!r}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\syosh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:271\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m    264\u001b[0m       warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    265\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    266\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    267\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    268\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    269\u001b[0m     fn_factory \u001b[39m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[1;32m--> 271\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function \u001b[39m=\u001b[39m fn_factory()\n\u001b[0;32m    272\u001b[0m \u001b[39m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[0;32m    273\u001b[0m add_to_graph \u001b[39m&\u001b[39m\u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[1;32mc:\\Users\\syosh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2610\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2601\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_concrete_function\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   2602\u001b[0m   \u001b[39m\"\"\"Returns a `ConcreteFunction` specialized to inputs and execution context.\u001b[39;00m\n\u001b[0;32m   2603\u001b[0m \n\u001b[0;32m   2604\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2608\u001b[0m \u001b[39m       or `tf.Tensor` or `tf.TensorSpec`.\u001b[39;00m\n\u001b[0;32m   2609\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2610\u001b[0m   graph_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_concrete_function_garbage_collected(\n\u001b[0;32m   2611\u001b[0m       \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   2612\u001b[0m   graph_function\u001b[39m.\u001b[39m_garbage_collector\u001b[39m.\u001b[39mrelease()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   2613\u001b[0m   \u001b[39mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32mc:\\Users\\syosh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2576\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2574\u001b[0m   args, kwargs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   2575\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m-> 2576\u001b[0m   graph_function, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n\u001b[0;32m   2577\u001b[0m   seen_names \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[0;32m   2578\u001b[0m   captured \u001b[39m=\u001b[39m object_identity\u001b[39m.\u001b[39mObjectIdentitySet(\n\u001b[0;32m   2579\u001b[0m       graph_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39minternal_captures)\n",
      "File \u001b[1;32mc:\\Users\\syosh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2760\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2758\u001b[0m   \u001b[39m# Only get placeholders for arguments, not captures\u001b[39;00m\n\u001b[0;32m   2759\u001b[0m   args, kwargs \u001b[39m=\u001b[39m placeholder_dict[\u001b[39m\"\u001b[39m\u001b[39margs\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m-> 2760\u001b[0m graph_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_graph_function(args, kwargs)\n\u001b[0;32m   2762\u001b[0m graph_capture_container \u001b[39m=\u001b[39m graph_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39m_capture_func_lib  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   2763\u001b[0m \u001b[39m# Maintain the list of all captures\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\syosh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2670\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2665\u001b[0m missing_arg_names \u001b[39m=\u001b[39m [\n\u001b[0;32m   2666\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (arg, i) \u001b[39mfor\u001b[39;00m i, arg \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(missing_arg_names)\n\u001b[0;32m   2667\u001b[0m ]\n\u001b[0;32m   2668\u001b[0m arg_names \u001b[39m=\u001b[39m base_arg_names \u001b[39m+\u001b[39m missing_arg_names\n\u001b[0;32m   2669\u001b[0m graph_function \u001b[39m=\u001b[39m ConcreteFunction(\n\u001b[1;32m-> 2670\u001b[0m     func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[0;32m   2671\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[0;32m   2672\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_function,\n\u001b[0;32m   2673\u001b[0m         args,\n\u001b[0;32m   2674\u001b[0m         kwargs,\n\u001b[0;32m   2675\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_signature,\n\u001b[0;32m   2676\u001b[0m         autograph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph,\n\u001b[0;32m   2677\u001b[0m         autograph_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph_options,\n\u001b[0;32m   2678\u001b[0m         arg_names\u001b[39m=\u001b[39;49marg_names,\n\u001b[0;32m   2679\u001b[0m         capture_by_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_capture_by_value),\n\u001b[0;32m   2680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_attributes,\n\u001b[0;32m   2681\u001b[0m     spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_spec,\n\u001b[0;32m   2682\u001b[0m     \u001b[39m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[0;32m   2683\u001b[0m     \u001b[39m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[0;32m   2684\u001b[0m     \u001b[39m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[0;32m   2685\u001b[0m     \u001b[39m# ConcreteFunction.\u001b[39;00m\n\u001b[0;32m   2686\u001b[0m     shared_func_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m   2687\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32mc:\\Users\\syosh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1153\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m   1150\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1151\u001b[0m   deps_control_manager \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mNullContextmanager()\n\u001b[1;32m-> 1153\u001b[0m \u001b[39mwith\u001b[39;00m func_graph\u001b[39m.\u001b[39mas_default(), deps_control_manager \u001b[39mas\u001b[39;00m deps_ctx:\n\u001b[0;32m   1154\u001b[0m   current_scope \u001b[39m=\u001b[39m variable_scope\u001b[39m.\u001b[39mget_variable_scope()\n\u001b[0;32m   1155\u001b[0m   default_use_resource \u001b[39m=\u001b[39m current_scope\u001b[39m.\u001b[39muse_resource\n",
      "File \u001b[1;32mc:\\Users\\syosh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\auto_control_deps.py:464\u001b[0m, in \u001b[0;36mAutomaticControlDependencies.__exit__\u001b[1;34m(self, unused_type, unused_value, unused_traceback)\u001b[0m\n\u001b[0;32m    461\u001b[0m resource_inputs \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[0;32m    462\u001b[0m \u001b[39m# Check for any resource inputs. If we find any, we update control_inputs\u001b[39;00m\n\u001b[0;32m    463\u001b[0m \u001b[39m# and last_write_to_resource.\u001b[39;00m\n\u001b[1;32m--> 464\u001b[0m \u001b[39mfor\u001b[39;00m inp, resource_type \u001b[39min\u001b[39;00m _get_resource_inputs(op):\n\u001b[0;32m    465\u001b[0m   is_read \u001b[39m=\u001b[39m resource_type \u001b[39m==\u001b[39m ResourceType\u001b[39m.\u001b[39mREAD_ONLY\n\u001b[0;32m    466\u001b[0m   input_id \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mtensor_id(inp)\n",
      "File \u001b[1;32mc:\\Users\\syosh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\auto_control_deps.py:662\u001b[0m, in \u001b[0;36m_get_resource_inputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m    660\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_resource_inputs\u001b[39m(op):\n\u001b[0;32m    661\u001b[0m   \u001b[39m\"\"\"Returns an iterable of resources touched by this `op`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 662\u001b[0m   reads, writes \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39;49mget_read_write_resource_inputs(op)\n\u001b[0;32m    663\u001b[0m   saturated \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    664\u001b[0m   \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m saturated:\n",
      "File \u001b[1;32mc:\\Users\\syosh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\auto_control_deps_utils.py:108\u001b[0m, in \u001b[0;36mget_read_write_resource_inputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m    105\u001b[0m   read_only_input_indices \u001b[39m=\u001b[39m op\u001b[39m.\u001b[39mget_attr(READ_ONLY_RESOURCE_INPUTS_ATTR)\n\u001b[0;32m    106\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m    107\u001b[0m   \u001b[39m# Attr was not set. Add all resource inputs to `writes` and return.\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m   writes\u001b[39m.\u001b[39mupdate(t \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m op\u001b[39m.\u001b[39;49minputs \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m dtypes\u001b[39m.\u001b[39mresource)\n\u001b[0;32m    109\u001b[0m   \u001b[39mreturn\u001b[39;00m (reads, writes)\n\u001b[0;32m    111\u001b[0m read_only_index \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\syosh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2522\u001b[0m, in \u001b[0;36mOperation.inputs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2518\u001b[0m \u001b[39m\"\"\"The sequence of `Tensor` objects representing the data inputs of this op.\"\"\"\u001b[39;00m\n\u001b[0;32m   2519\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inputs_val \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2520\u001b[0m   \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   2521\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inputs_val \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\n\u001b[1;32m-> 2522\u001b[0m       \u001b[39mmap\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgraph\u001b[39m.\u001b[39;49m_get_tensor_by_tf_output,\n\u001b[0;32m   2523\u001b[0m           pywrap_tf_session\u001b[39m.\u001b[39;49mGetOperationInputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_c_op)))\n\u001b[0;32m   2524\u001b[0m   \u001b[39m# pylint: enable=protected-access\u001b[39;00m\n\u001b[0;32m   2525\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inputs_val\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "seqlen = 40\n",
    "step = seqlen\n",
    "sentences = []\n",
    "for i in range(40, len(text) - seqlen - 1, step):\n",
    "    sentences.append(text[i: i + seqlen + 1])\n",
    "    \n",
    "\n",
    "x = np.zeros((len(sentences), seqlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), seqlen, len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, (char_in, char_out) in enumerate(zip(sentence[:-1], sentence[1:])):\n",
    "        x[i, t, char_indices[char_in]] = 1\n",
    "        y[i, t, char_indices[char_out]] = 1\n",
    "        \n",
    "        \n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(seqlen, len(chars)), return_sequences=True))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=RMSprop(learning_rate=0.01),\n",
    "    metrics=['categorical_crossentropy', 'accuracy']\n",
    ")\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    \"\"\"Helper function to sample an index from a probability array.\"\"\"\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.exp(np.log(preds) / temperature)  # softmax\n",
    "    preds = preds / np.sum(preds)                #\n",
    "    probas = np.random.multinomial(1, preds, 1)  # sample index\n",
    "    return np.argmax(probas)                     #\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    \"\"\"Function invoked at end of each epoch. Prints generated text.\"\"\"\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - seqlen - 1)\n",
    "    \n",
    "    for diversity in [0.5]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + seqlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, seqlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "            \n",
    "            preds = model.predict(x_pred, verbose=0)\n",
    "            next_index = sample(preds[0, -1], diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            \n",
    "            \n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          callbacks=[print_callback])\n",
    "          \n",
    "#model.fit(x, y,\n",
    "          #batch_size=128,\n",
    "          #epochs=150,\n",
    "          #callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file2 = open(\"nietzsche.txt\", encoding=\"utf-8\")\n",
    "text2 = file2.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text2)))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "seqlen = 40\n",
    "step = seqlen\n",
    "sentences = []\n",
    "for i in range(40, len(text2) - seqlen - 1, step):\n",
    "    sentences.append(text2[i: i + seqlen + 1])\n",
    "    \n",
    "\n",
    "x = np.zeros((len(sentences), seqlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), seqlen, len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, (char_in, char_out) in enumerate(zip(sentence[:-1], sentence[1:])):\n",
    "        x[i, t, char_indices[char_in]] = 1\n",
    "        y[i, t, char_indices[char_out]] = 1\n",
    "        \n",
    "        \n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(seqlen, len(chars)), return_sequences=True))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=RMSprop(learning_rate=0.01),\n",
    "    metrics=['categorical_crossentropy', 'accuracy']\n",
    ")\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    \"\"\"Helper function to sample an index from a probability array.\"\"\"\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.exp(np.log(preds) / temperature)  # softmax\n",
    "    preds = preds / np.sum(preds)                #\n",
    "    probas = np.random.multinomial(1, preds, 1)  # sample index\n",
    "    return np.argmax(probas)                     #\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    \"\"\"Function invoked at end of each epoch. Prints generated text.\"\"\"\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text2) - seqlen - 1)\n",
    "    \n",
    "    for diversity in [0.5]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text2[start_index: start_index + seqlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, seqlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "            \n",
    "            preds = model.predict(x_pred, verbose=0)\n",
    "            next_index = sample(preds[0, -1], diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            \n",
    "            \n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=120,\n",
    "          epochs=10,\n",
    "          callbacks=[print_callback])\n",
    "          \n",
    "#model.fit(x, y,\n",
    "          #batch_size=128,\n",
    "          #epochs=150,\n",
    "          #callbacks=[print_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8301d53dd684b00c73832b447ead7c53135833981e6900763d37ba09a4e6db7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
